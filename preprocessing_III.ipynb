{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing III\n",
    "\n",
    "This notebook extends the data output from preprocessing II, making it ready to train models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "path = './data/preprocessing_II'\n",
    "files = glob.glob(path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data\n",
    "\n",
    "Data is read from preprocessing II output files. For a quick analysis of the data distribution, the fraction of patients suffering from kidney issues is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def readFiles(csv_files):\n",
    "    return [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "ethn_list = readFiles(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print ethnicity order and percentage of patients suffering from kidney disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t22% \tamerican_indian_alaska_native\n",
      "1\t11% \tasian\n",
      "2\t22% \tblack_african_american\n",
      "3\t15% \thispanic_latino\n",
      "4\t14% \tother\n",
      "5\t7% \tunable_to_obtain\n",
      "6\t14% \tunknown\n",
      "7\t18% \twhite\n"
     ]
    }
   ],
   "source": [
    "def printEthnicities(csv_files, df_list):\n",
    "    for counter in range(len(csv_files)):\n",
    "        print(counter.__str__() + \"\\t\"  + (int(sum(df_list[counter]['has_kidney_issue'])/len(df_list[counter])*100)).__str__() + \"% \\t\" + csv_files[counter].split(\"\\\\\")[1].split(\".\")[0])\n",
    "\n",
    "printEthnicities(files, ethn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getTestDf(df_list):\n",
    "    return df_list[0]\n",
    "\n",
    "df = getTestDf(ethn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing types of columns and categorical variables\n",
    "\n",
    "The columns for 'emar_events', 'lab_priority' and 'lab_comments' are dropped as these do not contain interpretable data with predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'icd_code_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\annik\\Documenten\\Zweden\\KTH\\S1\\Research Methodology\\MIMIC-IV\\preprocessing_III.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dfs\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m ethn_list \u001b[39m=\u001b[39m convert_datetime(ethn_list, datetime)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m ethn_list \u001b[39m=\u001b[39m convert_int(ethn_list, ints)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m ethn_list \u001b[39m=\u001b[39m oneHot(ethn_list, one_hots)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m ethn_list \u001b[39m=\u001b[39m dropCols(ethn_list, drop_cols)\n",
      "\u001b[1;32md:\\Users\\annik\\Documenten\\Zweden\\KTH\\S1\\Research Methodology\\MIMIC-IV\\preprocessing_III.ipynb Cell 9\u001b[0m in \u001b[0;36mconvert_int\u001b[1;34m(dfs, cols)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         df \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         df \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/annik/Documenten/Zweden/KTH/S1/Research%20Methodology/MIMIC-IV/preprocessing_III.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dfs\n",
      "File \u001b[1;32md:\\Users\\annik\\anaconda3\\envs\\sklearn-env\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\annik\\anaconda3\\envs\\sklearn-env\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32md:\\Users\\annik\\anaconda3\\envs\\sklearn-env\\lib\\site-packages\\pandas\\core\\indexes\\range.py:389\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    390\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_loc(key, method\u001b[39m=\u001b[39mmethod, tolerance\u001b[39m=\u001b[39mtolerance)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'icd_code_count'"
     ]
    }
   ],
   "source": [
    "datetime = ['admittime', 'edregtime', 'emar_charttime', 'lab_charttime']\n",
    "ints = ['icd_code_count', 'emar_count', 'lab_count']\n",
    "one_hots = ['admission_type', 'admission_location', 'insurance', 'marital_status', 'gender']\n",
    "drop_cols = ['ethnicity', 'emar_events', 'lab_priority', 'lab_comments', 'lab_flag', 'Unnamed: 0', 'hadm_id', 'subject_id', 'edregtime', 'emar_charttime', 'lab_charttime']\n",
    "\n",
    "def convert_int(dfs, cols):\n",
    "    for df in dfs:\n",
    "        for col in cols:\n",
    "            df = df[col].fillna(0)\n",
    "            df = df[col].astype('int64')\n",
    "    return dfs\n",
    "\n",
    "def convert_datetime(dfs, cols):\n",
    "    for df in dfs:\n",
    "        for col in cols:\n",
    "            df[col] = pd.to_numeric(pd.to_datetime(df[col]))\n",
    "    return dfs\n",
    "\n",
    "def findAllUniques(dfs, col):\n",
    "    uniques = set()\n",
    "    for df in dfs:\n",
    "        for item in df[col].unique():\n",
    "            uniques.add(item)\n",
    "    return uniques\n",
    "\n",
    "def oneHot(dfs, cols):\n",
    "    for col in cols:\n",
    "        vals = findAllUniques(dfs, col)\n",
    "        for df in range(len(dfs)):\n",
    "            if len(vals) > 2:\n",
    "                for val in vals:\n",
    "                    dfs[df][val] = np.where(dfs[df][col]==val, True, False)\n",
    "\n",
    "                dfs[df] = dfs[df].drop(col, axis=1)\n",
    "            else:\n",
    "                print(dfs[df][col])\n",
    "                dfs[df][col] = np.where(dfs[df][col]==val[0], True, False)\n",
    "    # for df in dfs:\n",
    "    #     for col in cols:\n",
    "    #         dfs[df] = pd.get_dummies(df, columns=[col])\n",
    "    return dfs\n",
    "\n",
    "def dropCols(dfs, cols):\n",
    "    for col in cols:\n",
    "        for df in range(len(dfs)):\n",
    "            dfs[df] = dfs[df].drop(col, axis=1)\n",
    "    return dfs\n",
    "\n",
    "ethn_list = convert_datetime(ethn_list, datetime)\n",
    "ethn_list = convert_int(ethn_list, ints)\n",
    "ethn_list = oneHot(ethn_list, one_hots)\n",
    "ethn_list = dropCols(ethn_list, drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process list variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### emar_medication\n",
    "This column is processed by finding the most common 5 medications among kidney disease patients.\n",
    "\n",
    "These five are then one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def getLiterals(dfs, cols):\n",
    "    chars = \"[]''\"\n",
    "    for col in cols:\n",
    "        for df in range(len(dfs)):\n",
    "            dfs[df][col] = dfs[df][col].fillna(\"[]\")\n",
    "            for char in chars:\n",
    "                dfs[df][col] = dfs[df][col].apply(lambda x: x.replace(char, \"\"))\n",
    "            dfs[df][col] = dfs[df][col].apply(lambda x: x.split(\", \"))\n",
    "    return dfs\n",
    "\n",
    "def getUniquesInList(dfs, col):\n",
    "    uniques = set()\n",
    "    for df in range(len(dfs)):\n",
    "        kid_iss = dfs[df][dfs[df]['has_kidney_issue'] == True]\n",
    "        for item in kid_iss[col]:\n",
    "                for sub in item:\n",
    "                    uniques.add(sub)\n",
    "    return uniques\n",
    "\n",
    "def countMeds(dfs):\n",
    "    uniques = getUniquesInList(dfs, 'emar_medications')\n",
    "    meds = [0]*len(uniques)\n",
    "    # for every medicine found in patients with kidney issues\n",
    "    for i, med in enumerate(uniques):\n",
    "        # for every ethnicity\n",
    "        for df in range(len(dfs)):\n",
    "            kid_iss = dfs[df][dfs[df]['has_kidney_issue'] == True]\n",
    "            # for every patient's medications\n",
    "            for item in kid_iss['emar_medications']:\n",
    "                # if it contains the medicine, increment\n",
    "                if med in item:\n",
    "                    meds[i] += 1\n",
    "    return meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ethn_list = getLiterals(ethn_list, ['emar_medications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kidney_medications = getUniquesInList(ethn_list, 'emar_medications')\n",
    "kidney_medications = list(kidney_medications)\n",
    "\n",
    "med_count = np.genfromtxt('med_frequency.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# N most frequent meds\n",
    "N = 5\n",
    "\n",
    "inds = np.argsort(med_count)\n",
    "most_frequent = inds[-1]\n",
    "most_frequent = np.append(most_frequent, inds[-3])\n",
    "most_frequent = np.append(most_frequent, inds[-(N+2):-4])\n",
    "\n",
    "freq_med_names = []\n",
    "\n",
    "for item in most_frequent:\n",
    "    freq_med_names.append(kidney_medications[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def oneHotMeds(dfs, meds):\n",
    "    for med in meds:\n",
    "        for df in range(len(dfs)):\n",
    "            dfs[df][med] = False\n",
    "            for row in dfs[df].itertuples():\n",
    "                dfs[df].at[row.Index, med] = med in row.emar_medications\n",
    "    dfs = dropCols(dfs, ['emar_medications'])\n",
    "    return dfs\n",
    "\n",
    "ethn_list = oneHotMeds(ethn_list, freq_med_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for df in ethn_list:\n",
    "    df['admittime'] = pd.to_numeric(pd.to_datetime(df['admittime']))\n",
    "    df['icd_code_count'] = df['icd_code_count'].fillna(0)\n",
    "    df['emar_count'] = df['emar_count'].fillna(0)\n",
    "    df['lab_count'] = df['lab_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Abnormal lab flags\n",
    "The 'lab_flag' column is processed by counting the number of abnormal states\n",
    "\n",
    "EDIT: lab_flag column is dropped as its information is already encoded as lab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def countAbnormal(dfs):\n",
    "    for df in range(len(dfs)):\n",
    "        for row in dfs[df].itertuples():\n",
    "                print(row.lab_flag.count('abnormal'))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'sklearn-env (Python 3.10.6)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_name = ['american_indian_alaska_native', 'asian', 'black_african_american', 'hispanic_latino', 'other', 'unable_to_obtain', 'unknown', 'white']\n",
    "\n",
    "for df in range(len(ethn_list)):\n",
    "    print(\"Saving \", df_name[df], \"...\")\n",
    "    ethn_list[df].to_csv(\"data/preprocessing_III/\" + df_name[df] + \".csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sklearn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "34e7f5b862476ca5b132d6fa3a551e06a58745cd7626854c77bc546bf08a4906"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
