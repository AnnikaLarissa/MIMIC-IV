{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "This is a fast sklearn classifier, to investigate the model performance in the exploration phase. Further tuning of hyperparameters must be done. Also, the data needs further preprocessing, so we leave out all the object type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import glob\n",
    "\n",
    "ethnic_group_names = ['unknown', 'white', 'other', 'asian', 'hispanic_latino', 'black_african_american', 'unable_to_obtain', 'american_indian_alaska_native']\n",
    "path = './data/preprocessing_IV/'\n",
    "\n",
    "def read_train_val_test(name, path):\n",
    "    train = pd.read_csv(path + name + '_train.csv')\n",
    "    # NOTE: drope datetime columns for now, because sklearn does not support it\n",
    "    train = train.drop(columns=['admittime', 'edregtime', 'emar_charttime', 'lab_charttime'])\n",
    "    validate = pd.read_csv(path + name + '_validate.csv')\n",
    "    # NOTE: drope datetime columns for now, because sklearn does not support it\n",
    "    validate = validate.drop(columns=['admittime', 'edregtime', 'emar_charttime', 'lab_charttime'])\n",
    "    test = pd.read_csv(path + name + '_test.csv')\n",
    "    # NOTE: drope datetime columns for now, because sklearn does not support it\n",
    "    test = test.drop(columns=['admittime', 'edregtime', 'emar_charttime', 'lab_charttime'])\n",
    "    return train, validate, test\n",
    "\n",
    "def readHyperparams():\n",
    "    return pd.read_csv('hyperparameters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split features and labels\n",
    "def splitFeaturesLabels(df):\n",
    "    X = df.copy()\n",
    "    X = X.drop('has_kidney_issue', axis=1)\n",
    "    # X = df[['anchor_age', 'anchor_year']]\n",
    "    return X, df.has_kidney_issue"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train each local model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getBestParams(hyperparams, ethnicity):\n",
    "    # get the hyperparameters corresponding to the ethnicity\n",
    "    return hyperparams.loc[hyperparams['ethnicity']==ethnicity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trainModel(name, path, all_params):\n",
    "    # train a model with the specified params\n",
    "    train, val, test = read_train_val_test(name, path)\n",
    "    hyperparams = getBestParams(all_params, name)\n",
    "    X_train, y_train = splitFeaturesLabels(train)\n",
    "    clf = RandomForestClassifier(n_estimators=int(hyperparams['n_estimators']),\n",
    "                                 max_depth=int(hyperparams['max_depth']),\n",
    "                                 random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "params = readHyperparams()\n",
    "for eth in ethnic_group_names:\n",
    "    models.append(trainModel(eth, path, params))\n",
    "print(len(models))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregate models into federated forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def aggregateForests(agg, model, n):\n",
    "    # Aggregate the estimators of\n",
    "    size = model.n_estimators\n",
    "    dupl = math.floor(n/size)\n",
    "    for i in range(dupl):\n",
    "        agg.estimators_ += model.estimators_\n",
    "        agg.n_estimators = len(agg.estimators_)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def getMaxForest(model_list):\n",
    "    # normalize over the maximum number of trees so each model is weighted the same in the aggregated model\n",
    "    sizes = [len(model.estimators_) for model in model_list]\n",
    "    return max(sizes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def federatedForest(model_list):\n",
    "    # Aggregate all models and normalize\n",
    "    ff = model_list[0]\n",
    "    max_size = getMaxForest(model_list)\n",
    "    for i, model in enumerate(model_list):\n",
    "        if i > 0:\n",
    "            ff = aggregateForests(ff, model_list[i], max_size)\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed = federatedForest(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def evaluateMetrics(model, X, y):\n",
    "    y_pr = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pr)\n",
    "    precision = precision_score(y, y_pr)\n",
    "    recall = recall_score(y, y_pr)\n",
    "    auc = roc_auc_score(y, y_pr)\n",
    "    return accuracy, precision, recall, auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for each local model and the federated model, evaluate metrics like this:\n",
    "\n",
    "def printMetrics(model, ethnicity, path):\n",
    "    train, val, test = read_train_val_test(ethnicity, path)\n",
    "    X, y = splitFeaturesLabels(test)\n",
    "    acc, prec, rec, auc = evaluateMetrics(model, X, y)\n",
    "    print('Local model:\\t\\t' + ethnicity)\n",
    "    print('Accuracy: \\t\\t\\t' + str(acc))\n",
    "    print('Precision: \\t\\t\\t' + str(prec))\n",
    "    print('Recall: \\t\\t\\t' + str(rec))\n",
    "    print('AUC: \\t\\t\\t\\t' + str(auc) + '\\n\\n')\n",
    "\n",
    "def printFedMetrics(model, ethnicity, path):\n",
    "    train, val, test = read_train_val_test(ethnicity, path)\n",
    "    X, y = splitFeaturesLabels(test)\n",
    "    acc, prec, rec, auc = evaluateMetrics(model, X, y)\n",
    "    print('Federated model: \\t' + ethnicity)\n",
    "    print('Accuracy: \\t\\t\\t' + str(acc))\n",
    "    print('Precision: \\t\\t\\t' + str(prec))\n",
    "    print('Recall: \\t\\t\\t' + str(rec))\n",
    "    print('AUC: \\t\\t\\t\\t' + str(auc) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model:\t\tunknown\n",
      "Accuracy: \t\t\t0.8856848609680742\n",
      "Precision: \t\t\t0.6822033898305084\n",
      "Recall: \t\t\t0.5227272727272727\n",
      "AUC: \t\t\t\t0.7384138199621676\n",
      "\n",
      "\n",
      "Federated model: \tunknown\n",
      "Accuracy: \t\t\t0.8764160659114315\n",
      "Precision: \t\t\t0.6055900621118012\n",
      "Recall: \t\t\t0.6331168831168831\n",
      "AUC: \t\t\t\t0.7776967524519544\n",
      "\n",
      "\n",
      "Local model:\t\twhite\n",
      "Accuracy: \t\t\t0.8174476393326233\n",
      "Precision: \t\t\t0.721097615834458\n",
      "Recall: \t\t\t0.394002703699152\n",
      "AUC: \t\t\t\t0.6728458214019195\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m      7\u001B[0m     printMetrics(models[i], ethnic_group_names[i], path)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mprintFedMetrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43methnic_group_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36mprintFedMetrics\u001B[1;34m(model, ethnicity, path)\u001B[0m\n\u001B[0;32m     14\u001B[0m train, val, test \u001B[38;5;241m=\u001B[39m read_train_val_test(ethnicity, path)\n\u001B[0;32m     15\u001B[0m X, y \u001B[38;5;241m=\u001B[39m splitFeaturesLabels(test)\n\u001B[1;32m---> 16\u001B[0m acc, prec, rec, auc \u001B[38;5;241m=\u001B[39m \u001B[43mevaluateMetrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFederated model: \u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m ethnicity)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(acc))\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mevaluateMetrics\u001B[1;34m(model, X, y)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluateMetrics\u001B[39m(model, X, y):\n\u001B[1;32m----> 2\u001B[0m     y_pr \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y, y_pr)\n\u001B[0;32m      4\u001B[0m     precision \u001B[38;5;241m=\u001B[39m precision_score(y, y_pr)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001B[0m, in \u001B[0;36mForestClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    788\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;124;03m    Predict class for X.\u001B[39;00m\n\u001B[0;32m    790\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    806\u001B[0m \u001B[38;5;124;03m        The predicted classes.\u001B[39;00m\n\u001B[0;32m    807\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 808\u001B[0m     proba \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    810\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    811\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_\u001B[38;5;241m.\u001B[39mtake(np\u001B[38;5;241m.\u001B[39margmax(proba, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:861\u001B[0m, in \u001B[0;36mForestClassifier.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    856\u001B[0m all_proba \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    857\u001B[0m     np\u001B[38;5;241m.\u001B[39mzeros((X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], j), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[0;32m    858\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39matleast_1d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_)\n\u001B[0;32m    859\u001B[0m ]\n\u001B[0;32m    860\u001B[0m lock \u001B[38;5;241m=\u001B[39m threading\u001B[38;5;241m.\u001B[39mLock()\n\u001B[1;32m--> 861\u001B[0m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_joblib_parallel_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequire\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msharedmem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_accumulate_prediction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_proba\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlock\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimators_\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m proba \u001B[38;5;129;01min\u001B[39;00m all_proba:\n\u001B[0;32m    871\u001B[0m     proba \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:640\u001B[0m, in \u001B[0;36m_accumulate_prediction\u001B[1;34m(predict, X, out, lock)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_accumulate_prediction\u001B[39m(predict, X, out, lock):\n\u001B[0;32m    634\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;124;03m    This is a utility function for joblib's Parallel.\u001B[39;00m\n\u001B[0;32m    636\u001B[0m \n\u001B[0;32m    637\u001B[0m \u001B[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001B[39;00m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;124;03m    complains that it cannot pickle it when placed there.\u001B[39;00m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 640\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    641\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m lock:\n\u001B[0;32m    642\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:972\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.predict_proba\u001B[1;34m(self, X, check_input)\u001B[0m\n\u001B[0;32m    970\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    971\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_X_predict(X, check_input)\n\u001B[1;32m--> 972\u001B[0m proba \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    975\u001B[0m     proba \u001B[38;5;241m=\u001B[39m proba[:, : \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    # Since 'unknown' is overwritten as the aggregated model, redefine it\n",
    "    if i == 0:\n",
    "        unknown = trainModel('unknown', path, params)\n",
    "        printMetrics(unknown, ethnic_group_names[i], path)\n",
    "    else:\n",
    "        printMetrics(models[i], ethnic_group_names[i], path)\n",
    "    printFedMetrics(fed, ethnic_group_names[i], path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sklearn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e7f5b862476ca5b132d6fa3a551e06a58745cd7626854c77bc546bf08a4906"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}